{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdmhafi/.conda/envs/tf_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from keras.optimizers import Adam\n",
    "#from keras.applications.mobilenetv2 import MobileNetV2\n",
    "from keras.applications import mobilenet\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    input_shape = (400,400,3)\n",
    "    model = Sequential()\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    " \n",
    "    # Block 2\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    " \n",
    "    # Block 3\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    " \n",
    "    # Block 4\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "    # Block 5\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    # Block 6\n",
    "    #model.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "    #model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # FC1\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    # FC2\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    #FC3\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createModel()\n",
    "#model.summary()\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train red dataset size :  631\n",
      "Train notred dataset size :  161\n",
      "Val red dataset size :  101\n",
      "Val notred dataset size :  31\n",
      "Total training dataset 792\n",
      "Total validation dataset 132\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "train_imgs_r = glob.glob('data/train/red/*')\n",
    "train_imgs_nr = glob.glob('data/train/notred/*')\n",
    "val_imgs_r = glob.glob('data/val/red/*')\n",
    "val_imgs_nr = glob.glob('data/val/notred/*')\n",
    "print ('Train red dataset size : ' , len(train_imgs_r))\n",
    "print ('Train notred dataset size : ' , len(train_imgs_nr))\n",
    "print ('Val red dataset size : ' , len(val_imgs_r))\n",
    "print ('Val notred dataset size : ' , len(val_imgs_nr))\n",
    "\n",
    "total_train = len(train_imgs_r) + len(train_imgs_nr)\n",
    "total_val   = len(val_imgs_r) + len(val_imgs_nr)\n",
    "\n",
    "\n",
    "print ('Total training dataset' , total_train)\n",
    "print ('Total validation dataset' , total_val)\n",
    "\n",
    "\n",
    "img_width, img_height = 400, 400\n",
    "path='data/'\n",
    "train_data_dir = path + \"train\"\n",
    "validation_data_dir =  path + \"val\"\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 100\n",
    "\n",
    "img_rows, img_cols = img_height, img_width\n",
    "# number of convolutional filters to use\n",
    "#nb_filters = 32\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 792 images belonging to 2 classes.\n",
      "Found 132 images belonging to 2 classes.\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Initiate the train and test generators with data Augumentation\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode = \"nearest\",\n",
    "                                   zoom_range = 0.3,\n",
    "                                   width_shift_range = 0.3,\n",
    "                                   height_shift_range=0.3,\n",
    "                                   rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  horizontal_flip = True,\n",
    "                                  fill_mode = \"nearest\",\n",
    "                                  zoom_range = 0.3,\n",
    "                                  width_shift_range = 0.0,\n",
    "                                  height_shift_range=0.0,\n",
    "                                  rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size = (img_height, img_width),\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                        target_size = (img_height, img_width),\n",
    "                                                        class_mode = \"categorical\")\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Save the model according to the conditions\n",
    "checkpoint = ModelCheckpoint(\"model_vgg_2.h5\", \n",
    "                             monitor='val_acc', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=False, \n",
    "                             mode='auto', \n",
    "                             period=1)\n",
    "\n",
    "early = EarlyStopping(monitor='val_acc', \n",
    "                      min_delta=0, \n",
    "                      patience=10, \n",
    "                      verbose=1, \n",
    "                      mode='auto')\n",
    "\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "792/792 [==============================] - 331s 418ms/step - loss: 0.3256 - acc: 0.8668 - val_loss: 0.1975 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.91304, saving model to model_vgg_2.h5\n",
      "Epoch 2/100\n",
      "792/792 [==============================] - 325s 410ms/step - loss: 0.1775 - acc: 0.9141 - val_loss: 0.1931 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.91304\n",
      "Epoch 3/100\n",
      "792/792 [==============================] - 324s 409ms/step - loss: 0.2009 - acc: 0.9021 - val_loss: 0.1898 - val_acc: 0.9145\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.91304 to 0.91447, saving model to model_vgg_2.h5\n",
      "Epoch 4/100\n",
      "792/792 [==============================] - 326s 411ms/step - loss: 0.1785 - acc: 0.9127 - val_loss: 0.1852 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.91447 to 0.91533, saving model to model_vgg_2.h5\n",
      "Epoch 5/100\n",
      "792/792 [==============================] - 326s 412ms/step - loss: 0.1800 - acc: 0.9115 - val_loss: 0.1905 - val_acc: 0.9108\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.91533\n",
      "Epoch 6/100\n",
      "792/792 [==============================] - 322s 406ms/step - loss: 0.1742 - acc: 0.9160 - val_loss: 0.1865 - val_acc: 0.9156\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.91533 to 0.91562, saving model to model_vgg_2.h5\n",
      "Epoch 7/100\n",
      "792/792 [==============================] - 328s 415ms/step - loss: 0.1696 - acc: 0.9198 - val_loss: 0.1830 - val_acc: 0.9165\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.91562 to 0.91648, saving model to model_vgg_2.h5\n",
      "Epoch 8/100\n",
      "792/792 [==============================] - 330s 417ms/step - loss: 0.1698 - acc: 0.9216 - val_loss: 0.1888 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.91648\n",
      "Epoch 9/100\n",
      "792/792 [==============================] - 326s 412ms/step - loss: 0.3998 - acc: 0.8341 - val_loss: 0.5529 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.91648\n",
      "Epoch 10/100\n",
      "792/792 [==============================] - 328s 414ms/step - loss: 0.5100 - acc: 0.7967 - val_loss: 0.5494 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.91648\n",
      "Epoch 11/100\n",
      "792/792 [==============================] - 326s 411ms/step - loss: 0.5092 - acc: 0.7967 - val_loss: 0.5517 - val_acc: 0.7657\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.91648\n",
      "Epoch 12/100\n",
      "792/792 [==============================] - 326s 411ms/step - loss: 0.5071 - acc: 0.7967 - val_loss: 0.5645 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.91648\n",
      "Epoch 13/100\n",
      "792/792 [==============================] - 329s 416ms/step - loss: 0.5083 - acc: 0.7967 - val_loss: 0.5526 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.91648\n",
      "Epoch 14/100\n",
      "792/792 [==============================] - 329s 415ms/step - loss: 0.5082 - acc: 0.7967 - val_loss: 0.5484 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.91648\n",
      "Epoch 15/100\n",
      "792/792 [==============================] - 323s 407ms/step - loss: 0.5074 - acc: 0.7967 - val_loss: 0.5466 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.91648\n",
      "Epoch 16/100\n",
      "792/792 [==============================] - 326s 411ms/step - loss: 0.5074 - acc: 0.7967 - val_loss: 0.5504 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.91648\n",
      "Epoch 17/100\n",
      "792/792 [==============================] - 328s 415ms/step - loss: 0.5062 - acc: 0.7967 - val_loss: 0.5459 - val_acc: 0.7654\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.91648\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch  = total_train,\n",
    "                    epochs = epochs,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = total_val,\n",
    "                    callbacks = [checkpoint, early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_path, show=False):\n",
    "\n",
    "    img = image.load_img(img_path, target_size=(400, 400))\n",
    "    img_tensor = image.img_to_array(img)                    # (height, width, channels)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)\n",
    "    img_tensor /= 255.                                      # imshow expects values in the range [0, 1]\n",
    "\n",
    "    if show:\n",
    "        plt.imshow(img_tensor[0])                           \n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "import glob\n",
    "not_reds = glob.glob('data/test/notred/*')\n",
    "for img in not_reds:\n",
    "    img_tensor = process_image(img)\n",
    "    pred = model.predict(img_tensor)\n",
    "    predicted_class = np.argmax(pred, axis=1)\n",
    "    \n",
    "    print ('Pedicted classes: ', predicted_class[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
